---
title: 缓存置换算法和LRU实现
date: 2020-08-14 15:33:30
tags:
- 算法 
categories:
- [算法, 经典算法]
---

# 缓存置换的常见策略和LRU算法实现

> 常见于缓存池更新方面的应用
> 内核设计的页面置换机制中有重要应用
<!-- more -->

#### 开篇

就算不是学计算机的，也会对缓存技术有所耳闻，包括“浏览器缓存”和“手机缓存垃圾”在内的各种名词，其实早就给你塑造了一个缓存的大概形象，本篇不会纠结缓存的定义到底是什么，主要是介绍缓存技术在计算机内存和程序设计方面的实现和一些常见案例，建立起对缓存技术的一个比较清晰的概念

#### 早期CPU的困境

缓存，英文Cache，最早出现于1967年的一篇电子设计论文，Cache原本是一个法语词，在论文中被作者用来指意```safekeeping storage```。在以前16位机的时代，CPU和内存都很慢，不存在什么Cache技术，运行指令时由CPU直接访问内存，到了80386的32位机时代，出现了Cache技术，众所周知，在现代计算机体系结构中，存储是拥有级别的，最快的存储单元是CPU的寄存器，同时也最昂贵，通常只负责存储会参与直接计算的数据和指令，其次是内存，内存比CPU寄存器慢，但是造价相对便宜，进入21世纪后，随着内存技术的升级，8GB的DDR4内存条也只要几百块就能买到了（而一颗拥有若干KB存储的Intel处理器就大几千了），最便宜的则是外部存储设备了，也就是常见的硬盘技术，大容量，造价便宜，但是数据读写也最慢。由此可知，在Cache技术出现之前，CPU里的寄存器虽然很快，但是程序的指令是加载在内存里的，CPU要执行指令，就要到内存中去读取一条条的指令，就算你寄存器计算时再快，但是内存的IO瓶颈在那里，CPU在等待内存返回指令之前就只等“干着急”，所以计算机的运行速度一直上不去

#### 救世主的到来：Cache技术

但是随着80386系列的处理器的出现，情况就不同了，80386的芯片组增加了对可选的Cache的支持，高级主板可以携带64KB。提供“缓存”的目的是为了让数据访问的速度适应CPU的处理速度，上文提过了，在Cache技术出现之前，CPU直接访问内存，内存如果慢了，CPU再快也要等内存，而出现了Cache之后，当CPU处理数据时，它会先到Cache中去寻找，如果数据因之前的操作已经读取而被暂存其中，那就直接读取Cache中的指令结果，这样一来，CPU再也不用每次都等“慢吞吞”的内存了，整体的计算效率也上去了

> Cache技术能够成功的原理之一，就是所谓的“程序执行的局域性原理”，用人话讲就是“一定程序执行时间和空间内，被访问的代码集中于一部分”，除了C语言之外，包括Java在内的很多编程语言都会使用Cache机制提高自己的执行速度，有兴趣的可以了解一下Java的voliate关键字

下图是Intel Core i5-4285U的CPU三级缓存示意图（随便看看就行了，本篇不是讲CPU的）：
<img src="/images/algorithm/cache/javabf_cpu_01.png" title="javabf_cpu_01" alt="javabf_cpu_01" style="max-width:80%;margin:auto;" />

#### 广义缓存技术

而在现在，缓存技术不再仅仅是指CPU的Cache了，而是在各类计算机问题中都找到了应用，我们称之为广义缓存技术（其实我们现在在大部分情况下提到的都是广义的缓存技术），根据我们上文了解到的Cache技术的历史，可以了解到所谓的Cache其实有**3个基本属性**：

1. 热数据
根据“局部性原理”，系统目前访问的数据A，极有可能在不久的未来二次访问，所以我们就将这个数据暂存起来，之后再访问的时候就免得再找一次了，直接从缓存中拿，提高了系统的整体速度。当CPU执行其它程序，或者当前热数据已经修改时，再通过一定的机制更新热数据，保证缓冲区存放的是“最要紧的数据”
2. 不对等存储
Cache的出现说到底是CPU的高速存储和内存的低速存储之间的读写速度不对等造成的，这个“不对等”现象其实非常常见，比如Redis作为Mysql的数据缓存，就是因为关系数据库每次读写都是基于磁盘的，非常慢，但是Redis作为键值数据库运行在内存中，大大提高了关系数据库中的“热数据”（比如某个用户的微博访问量）的访问速度，在高并发环境下是非常有效的策略
3. 有限容量
在讲CPU时提到了，不对等存储的出现，本质上还是存储设备的造价问题，所以你在高速设备（容量A）和低速设备（容量B）之间提供的缓存区（容量C）通常会有```A<C<B```这样的关系，所以这就导致了缓存技术能提供的容量也是有限的，当缓存容量用尽时，就会出现更新策略，也就是本文的重点之一：Cache Replacement Policies缓存置换机制

#### 缓存置换的4个基本策略

让我们继续回到富含激情的80年代，当时能使用的计算机多是16位机，比如经典的8086体系，拥有20位的地址总线，也就是说我们的内存其实只有2^20B=1MB的容量，但是数据文件可没有因为你是早期计算机就变小，假设你在当时的斯坦福大学，要检阅学校服务器上的一个2MB大小的学生信息文件，你怎么办？

通过上文的阅读，我们知道一个缓存机制的3个特点：```热数据```，```不对等存储```和```有限容量```在我们的这个场景里其实全有了，所以有了以下解决方案，也就是经典的“大文件局部加载方案”（又是一个比我年纪还大的算法）：

1. 将文件分片，2MB的文件分成2048片，每片1KB大小，只有当你的阅读器读到当前的文件片时，才将文件加载进内存，剩下的留在硬盘里
2. 计算机读文件时，会先检索内存中是否已经加载过目标文件片了，如果已加载，则直接从内存中取
3. 如果内存满了，那就按照**某些规则**，放弃一些“旧的”文件片，给新来的文件片挪位置

这里的**某些规则**其实就是缓存置换策略，常见策略一共4个：

1. FIFO：First In First Out，最先进入的内容作为替换对象
2. LRU：Least Recently Used，最久没有访问的内容作为替换对象
3. LFU：Least Frequently Used，最近最少使用的内容作为替换对象
4. MRU：Most Recently Used，最近使用的内容作为替换对象
> 其实策略还有非常多，取得应用的有几十个，这几个时属于比较常见比较常见的

#### Cache Hit/Miss和缓存命中率

还是上面的大文件分片加载的案例，既然缓存置换策略那么多，那么肯定会出现良莠不齐的情况，那么就需要一个衡量标准，下面接触两个概念：

- Cache Hit/Miss
Cache Hit又称缓存命中，当我们的程序发出了一个文件片的加载指令，我们会查询文件片缓存池中是否有这个文件片，如果有，那就表示Cache Hit了，我们直接从缓存中读取这个文件片，反之如果发现这个文件片不在缓存中，那么就表示Cache Miss，我们需要出发一个I/O读写任务，付出额外的时间代价去硬盘读取目标文件片
- 缓存命中率
根据Cache Hit/Miss的定义，我们自然是希望Cache Hit的情况多于Miss的情况，Hit越多，说明我们的缓存确实生效了，所以出现了缓存命中率的公式：```Hit Rate=(Cache Hit)/(Cache Hit+Cache Miss)```

假设我们的缓存区容量为：3个文件片，现在我们的程序发出的读写序列为```{7,0,1,2,0,3,0,4,2,3,0,3,2,1,2,0,1,7,0,1}```，其中的数字代表文件片编号：

现在我们构造了一个空表，可用来理解各个策略在实施时的不同：

| 请求序号 |  7  |  0  |  1  |  2  |  0  |  3  |  0  |  4  |  2  |  3  |  0  |  3  |  2  |  1  |  2  |  0  |  1  |  7  |  0  |  1  |
| ------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 缓存片0  |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |
| 缓存片1  |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |
| 缓存片2  |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |
| 结果     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |
| 读取     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |
| 丢弃     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |
> 结果一栏是指是否命中，H=Cache Hit && M=Cache Miss

###### FIFO：先进先出算法

在这个策略中，发生Cache Miss时，会优先丢弃**最早进入缓存**的文件片，也就是说每次都淘汰当前缓存中在内存中驻留时间最长的那个文件片。在本例中，Hit了5次，Miss了15次，命中率为25%

| 请求序号 |  7  |  0  |  1  |  2  |  0  |  3  |  0  |  4  |  2  |  3  |  0  |  3  |  2  |  1  |  2  |  0  |  1  |  7  |  0  |  1  |
| ------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 缓存片0  | 7   | 7   | 7   | 2   | 2   | 2   | 2   | 4   | 4   | 4   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 7   | 7   | 7   |
| 缓存片1  |     | 0   | 0   | 0   | 0   | 3   | 3   | 3   | 2   | 2   | 2   | 2   | 2   | 1   | 1   | 1   | 1   | 1   | 0   | 0   |
| 缓存片2  |     |     | 1   | 1   | 1   | 1   | 0   | 0   | 0   | 3   | 3   | 3   | 3   | 3   | 2   | 2   | 2   | 2   | 2   | 1   |
| 结果     | M   | M   | M   | M   | H   | M   | M   | M   | M   | M   | M   | H   | H   | M   | M   | H   | H   | M   | M   | M   |
| 读取     | 7   | 0   | 1   | 2   |     | 3   | 0   | 4   | 2   | 3   | 0   |     |     | 1   | 2   |     |     | 7   | 0   | 1   |
| 丢弃     |     |     |     | 7   |     | 0   | 1   | 2   | 3   | 0   | 4   |     |     | 2   | 3   |     |     | 0   | 1   | 2   |

###### LRU：最近久未用算法

发生Cache时，会检索使用记录，保留最近用过的文件片（也就是丢弃**最近没用过**的文件片），这个其实是局部性原理的反应，程序认为过去一段时间内不曾被访问的页面，在最近的将来也不会被访问，所以它总是丢弃最近一段时间内最久不用的文件片。在本例中，Hit了10次，Miss了10次，命中率为50%

| 请求序号 |  7  |  0  |  1  |  2  |  0  |  3  |  0  |  4  |  2  |  3  |  0  |  3  |  2  |  1  |  2  |  0  |  1  |  7  |  0  |  1  |
| ------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 缓存片0  | 7   | 7   | 7   | 2   | 2   | 2   | 2   | 4   | 4   | 4   | 0   | 0   | 0   | 1   | 1   | 1   | 1   | 1   | 1   | 1   |
| 缓存片1  |     | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 3   | 3   | 3   | 3   | 3   | 3   | 0   | 0   | 0   | 0   | 0   |
| 缓存片2  |     |     | 1   | 1   | 1   | 3   | 3   | 3   | 2   | 2   | 2   | 2   | 2   | 2   | 2   | 2   | 2   | 7   | 7   | 7   |
| 结果     | M   | M   | M   | M   | H   | M   | H   | M   | M   | H   | M   | H   | H   | M   | H   | H   | H   | M   | H   | H   |
| 读取     | 7   | 0   | 1   | 2   |     | 3   |     | 4   | 2   |     | 0   |     |     | 1   |     |     |     | 7   |     |     |
| 丢弃     |     |     |     | 7   |     | 1   |     | 2   | 3   |     | 4   |     |     | 0   |     |     |     | 2   |     |     |
> 这里是为了画表方便，正式编写时，缓存片的存储顺序会稍有不同，下文讲实现时会提及

###### LFU：最近低频淘汰算法

这个策略有一点点类似LRU，但是这里衡量的并不是文件片最近有没有被使用（时间戳），而是在最近一段时间内，它被访问的次数，访问率低的文件片会被丢弃。在实现上，其实就是给每个文件片加一个计数器，访问了就喜加一，当出现缓存不够时，检索当前缓存中，计数器最小的那个，把它的计数器清零后踢出缓存（好惨）。这个算法其实经常和LRU一起使用，因为如果你发现目前缓存区里所有文件片的计数器都一个大小，那你就要考虑通过LRU来抉择了（如下图的第4次Miss）。在本例中，Hit了9次，Miss了11次，命中率为45%

| 请求序号 |  7   |  0   |  1   |  2   |  0   |  3   |  0   |  4   |  2   |  3   |  0   |  3   |  2   |  1   |  2   |  0   |  1   |  7   |  0   |  1   |
| ------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 缓存片0  | 7(1) | 7(1) | 7(1) | 2(1) | 2(1) | 2(1) | 2(1) | 4(1) | 4(1) | 3(1) | 3(1) | 3(2) | 3(2) | 1(1) | 1(1) | 1(1) | 1(2) | 7(1) | 7(1) | 1(1) |
| 缓存片1  |      | 0(1) | 0(1) | 0(1) | 0(2) | 0(2) | 0(3) | 0(3) | 0(3) | 0(3) | 0(4) | 0(4) | 0(4) | 0(4) | 0(4) | 0(5) | 0(5) | 0(5) | 0(6) | 0(6) |
| 缓存片2  |      |      | 1(1) | 1(1) | 1(1) | 3(1) | 3(1) | 3(1) | 2(1) | 2(1) | 2(1) | 2(1) | 2(2) | 2(2) | 2(3) | 2(3) | 2(3) | 2(3) | 2(3) | 2(3) |
| 结果     | M    | M    | M    | M    | H    | M    | H    | M    | M    | M    | H    | H    | H    | M    | H    | H    | H    | M    | H    | M    |
| 读取     | 7    | 0    | 1    | 2    |      | 3    |      | 4    | 2    | 3    |      |      |      | 1    |      |      |      | 7    |      | 1    |
| 丢弃     |      |      |      |      |      | 1    |      | 2    | 3    | 4    |      |      |      | 3    |      |      |      | 1    |      | 7    |
> 如果表太长了导致滚动条，请把浏览器窗口拉长
> 7(1)表示文件片7的计数器为1
> LFU算法有一个小问题，那就是如果某个文件片在载入缓冲区后被大量读写，然后程序就再也不访问了，这种情况下，就会因为之前大量访问给文件片制造了一个超大的计数器，就会变成“仗着祖上阔的恶霸”，导致“赖在缓存区不走”，所以正式使用时，计数器都是有时效的

###### MRU：最近使用淘汰算法

乍一听蛮奇怪的，当缓存区满时，这个策略会淘汰最近只用过的文件片，这不是给自己添堵？但其实这个在某些场合非常使用，比如你要对某个大型数据集进行循环扫描（又称循环访问模式，由于MRU缓存算法倾向于保留较旧的数据，因此它们比LRU的命中率高，这个算法的图留给观众老爷们自己画吧，当是测试了（其实就是作者偷懒）

#### 缓存置换策略的选择

上面是给出了4种基本的置换策略实现，但是可以看出，这些算法的效率是极度依赖输入队列的，也就是**读写序列**，在某些情况下，可能LRU命中率高，换个情况可能就是MRU高了，所以类似虚拟内存这种使用了缓存/页面置换算法的场景，其实是准备了多种策略，监控其命中率指标，根据系统任务的表现，可以做到随时切换，我们在设计自己的缓存系统时也可以这么考虑

而且默认环境下，我们一般会采用LRU+LFU的做法，在各种测试中，这一类的组合表现比较稳定

#### LRU Cache算法的实现

为了加强对算法的理解，我们来实现一下LRU策略的缓存置换算法，LRU的策略可能很简单，但是在各个场景（比如内核）下进行应用时会产生大量的变种，我们这里给出的示例属于“面试特供版”，因为企业在笔试时很喜欢问LRU一类的问题，考察对数据结构的掌握。我们这里只实现LRU，其它策略也基本大同小异，观众老爷可以自行摸索

在实际编写代码之前，我们来回顾一下，LRU的基本策略：

- 维护一个限制大小的缓存池
- 当缓存空间满了的时候，将最近**最久少使用**的数据从缓存空间中删除，以增加可用的缓存空间来缓存新的数据

###### LRU的核心数据结构

因为我们要定位**最久少使用**的数据，所以我们需要一个可以记录时间顺序的结构，在这里我们使用增强型的链表，也就是双向链表，现在确定一下我们的算法基本步骤：

- 设置一个双向链表visited_list模拟循环队列，用来记录数据的新旧（仅记录数据的编号），新数据插入到头部，旧数据从尾部丢弃
- 设置一个字典dict<数据编号:数据内容>作为真正的数据存储池
- 读取请求来后，访问字典dict来判定Cache Hit/Miss
- Cache Hit后，更新visited_list，Hit了的数据编号会被取出，插入到头部，这样确保从尾部丢弃的确实是**最久且最少**被使用的数据（因为新数据都被更新去头部了，而根据队列的特性，靠近尾部的都是老数据），从dict中返回对应的数据内容
- Cache Miss后，查询缓存是否富余，富余则直接向dict和visited_list插入新数据，不足则会删除visited_list的尾部数据，删除dict中对于的数据内容，将新数据调入并插入到visited_list头部和dict

完整的Java代码如下：

```java
package com.blade.app.lru;

import java.util.HashMap;

/**
 * 维护一个类似双向链表的数据结构，包含前驱后继以及键值结构
 */
class Node {
    Node next;
    Node prev;
    int key;
    int val;

    public Node(int key, int val) {
        this.key = key;
        this.val = val;
    }

    public Node() {
    }
}

class LRUCache {

    // 散列提供O(1)的读写
    HashMap<Integer, Node> map;

    // 头指针，head->next指向最近访问的结点
    Node head;
    // 尾指针，head->prev指向最久访问的结点
    Node tail;
    // 容量
    int capacity;
    // 结点计数
    int count;

    public LRUCache(int capacity) {
        // 计数
        this.capacity = capacity;
        this.count = 0;

        map = new HashMap<>();
        head = new Node();
        tail = new Node();

        // 循环队列
        head.next = tail;
        tail.next = head;
    }

    public int get(int key) {
        // 如果 key 在 HashMap 中，先拿到该结点，删除结点，再插入结点。
        if (map.containsKey(key)) {
            Node node = map.get(key);
            remove(node);
            insert(node);
            return map.get(key).val;
        }
        // 如果不在就返回 -1
        return -1;
    }

    public void put(int key, int value) {
        // 如果 key 在 HashMap 中，和 get 类似，也是先拿到该结点，删除结点，再插入结点。
        if (map.containsKey(key)) {
            Node node = map.get(key);
            node.val = value;
            remove(node);
            insert(node);
        } else {
            // 如果 key 不在 HashMap 中，那么是一个新的结点，直接插入即可。
            Node node = new Node(key, value);
            insert(node);
        }
    }

    public void remove(Node node) {
        if (count > 0) {
            // 在 map 中移除结点前，先将双向链表的指针指向进行修改
            Node prev = node.prev;
            Node next = node.next;
            node.prev = null;
            node.next = null;
            next.prev = prev;
            prev.next = next;
            map.remove(node.key);
            count--;
        }
    }

    public void insert(Node node) {
        Node next = head.next;
        head.next = node;
        node.next = next;
        next.prev = node;
        node.prev = head;
        map.put(node.key, node);
        count++;
        // 如果结点数超过可允许容量，将 least recently 的结点移除
        if (count > capacity) {
            remove(tail.prev);
        }
    }
}

public class TestLRUCache {

    public static void main(String[] args) {

        // 容量定为4
        LRUCache cache = new LRUCache(4);

        // 初始化节点
        Node[] nodes = new Node[6];
        for (int i = 0; i < 6; i++) {
            nodes[i] = new Node(i, i * 100);
        }

        // 定义访问序列
        int[] visit_q = {0, 1, 2, 4, 5, 2, 3, 4, 3, 0, 1, 4, 5, 3};
        for (int i : visit_q) {
            Node n = nodes[i];
            cache.put(n.key, n.val);
        }

        System.out.println("end.");
    }
}
```

在本例中，我使用的访问队列为```{0, 1, 2, 4, 5, 2, 3, 4, 3, 0, 1, 4, 5, 3}```，老规矩可以画出下面的过程图：

| 请求序号 |  0  |  1  |  2  |  4  |  5  |  2  |  3  |  4  |  3  |  0  |  1  |  4  |  5  |  3  |
| ------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 结点0    | 0   | 1   | 2   | 4   | 5   | 2   | 3   | 4   | 3   | 0   | 1   | 4   | 5   | 3   |
| 结点1    |     | 0   | 1   | 2   | 4   | 5   | 2   | 3   | 4   | 3   | 0   | 1   | 4   | 5   |
| 结点2    |     |     | 0   | 1   | 2   | 4   | 5   | 2   | 2   | 4   | 3   | 0   | 1   | 4   |
| 结点3    |     |     |     | 0   | 1   | 1   | 4   | 5   | 5   | 2   | 4   | 3   | 0   | 1   |
| 结果     | M   | M   | M   | M   | M   | H   | M   | H   | H   | M   | M   | H   | M   | M   |
> 这个图和之前的不太一样，因为这里的结点0-3时有顺序的，每次更新的数据都会被提到结点0的位置，而每次也都是从结点3淘汰数据
> 如果你仔细观察，会发现我们的LRU表有明显的阶梯型

#### 一点碎碎念

到此，“大文件局部加载”的案例就结束了，剩下还有一点碎碎念，有兴趣的朋友可以自己扩展一下：

- Linux中的虚拟内存概念就是基于页面置换算法（缓存置换的一种实现），通过这种方式，物理内存不再是程序运行的限制，假设你有8GB的物理内存，但是虚拟内存可以达到16GB，溢出的内存直接以交换区的形式放到磁盘上就行，但是时刻注意，置换算法是典型的时间换空间，你的内存虽然扩大了，但是调度是要吃CPU的。这种方式又叫“虚拟扩容”
- “抖动现象”是指数据频繁进出缓存区，如果针对某个的访问队列使用了不合适的置换算法，就会出现这种情况，命中率下降，消耗了CPU的同时，没有获得缓存的优势

#### 总结

到此为止，关于缓存置换算法就草草讲完了，实现只写了LRU的Java实现，感兴趣的可以去看看Linux内核的源码，里面对置换算法是真的玩出花了，如果感觉不太理解的话，可以自己去看一下wiki或者动手把表画一下。如果你发现了文章错误，记得在评论区指出QAQ

